{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abHb62yRXypM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a Support Vector Machine (SVM)?\n",
        "\n",
        "SVM is a supervised learning algorithm used for classification and regression that finds the optimal hyperplane separating data.\n",
        "\n",
        "Example: Classifying emails as spam or not spam.\n",
        "\n",
        "2. What is the difference between Hard Margin and Soft Margin SVM?\n",
        "\n",
        "Hard Margin: No misclassification allowed\n",
        "\n",
        "Soft Margin: Allows some misclassification to handle noisy data\n",
        "\n",
        "Example: Soft margin is used in real-world datasets.\n",
        "\n",
        "3. What is the mathematical intuition behind SVM?\n",
        "\n",
        "SVM maximizes the margin (distance) between different classes.\n",
        "\n",
        "4. What is the role of Lagrange Multipliers in SVM?\n",
        "\n",
        "They help solve the constrained optimization problem and identify support vectors.\n",
        "\n",
        "5. What are Support Vectors in SVM?\n",
        "\n",
        "Support vectors are data points closest to the hyperplane.\n",
        "\n",
        "Example: Boundary points between two classes.\n",
        "\n",
        "6. What is a Support Vector Classifier (SVC)?\n",
        "\n",
        "SVC is the classification version of SVM.\n",
        "\n",
        "7. What is a Support Vector Regressor (SVR)?\n",
        "\n",
        "SVR is used to predict continuous values.\n",
        "\n",
        "Example: Predicting house prices.\n",
        "\n",
        "8. What is the Kernel Trick in SVM?\n",
        "\n",
        "It converts non-linear data into higher dimensions for linear separation.\n",
        "\n",
        "9. Compare Linear Kernel, Polynomial Kernel, and RBF Kernel:\n",
        "\n",
        "Linear: Simple, linearly separable data\n",
        "\n",
        "Polynomial: Curved decision boundaries\n",
        "\n",
        "RBF: Highly non-linear data\n",
        "\n",
        "10. What is the effect of the C parameter in SVM?\n",
        "\n",
        "Controls trade-off between margin size and classification error.\n",
        "\n",
        "11. What is the role of the Gamma parameter in RBF Kernel SVM?\n",
        "\n",
        "Controls complexity of decision boundary.\n",
        "\n",
        "12. What is the Na√Øve Bayes classifier, and why is it called \"Na√Øve\"?\n",
        "\n",
        "It is a probabilistic classifier based on Bayes‚Äô theorem and is called ‚Äúna√Øve‚Äù due to the independence assumption.\n",
        "\n",
        "13. What is Bayes‚Äô Theorem?\n",
        "ùëÉ\n",
        "(\n",
        "ùê¥\n",
        "‚à£\n",
        "ùêµ\n",
        ")\n",
        "=\n",
        "ùëÉ\n",
        "(\n",
        "ùêµ\n",
        "‚à£\n",
        "ùê¥\n",
        ")\n",
        "ùëÉ\n",
        "(\n",
        "ùê¥\n",
        ")\n",
        "ùëÉ\n",
        "(\n",
        "ùêµ\n",
        ")\n",
        "P(A‚à£B)=\n",
        "P(B)\n",
        "P(B‚à£A)P(A)\n",
        "\t‚Äã\n",
        "\n",
        "14. Explain the differences between Gaussian, Multinomial, and Bernoulli Na√Øve Bayes:\n",
        "\n",
        "Gaussian: Continuous data\n",
        "\n",
        "Multinomial: Text data\n",
        "\n",
        "Bernoulli: Binary features\n",
        "\n",
        "15. When should you use Gaussian Na√Øve Bayes?\n",
        "\n",
        "When features are continuous and normally distributed.\n",
        "\n",
        "16. What are the key assumptions made by Na√Øve Bayes?\n",
        "\n",
        "Features are independent\n",
        "\n",
        "Each feature contributes equally\n",
        "\n",
        "17. What are the advantages and disadvantages of Na√Øve Bayes?\n",
        "\n",
        "Advantages: Fast, simple\n",
        "Disadvantages: Strong independence assumption\n",
        "\n",
        "18. Why is Na√Øve Bayes a good choice for text classification?\n",
        "\n",
        "Works well with high-dimensional sparse data.\n",
        "\n",
        "19. Compare SVM and Na√Øve Bayes for classification tasks:\n",
        "\n",
        "SVM: High accuracy, slow\n",
        "\n",
        "Na√Øve Bayes: Fast, less accurate\n",
        "\n",
        "20. How does Laplace Smoothing help in Na√Øve Bayes?\n",
        "\n",
        "Prevents zero probability for unseen features.\n",
        "\n"
      ],
      "metadata": {
        "id": "xpo4-9qVZkD7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Write a Python program to train an SVM Classifier on the Iris dataset and evaluate accuracy\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "model = SVC()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "2. Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset and compare their accuracies\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = load_wine(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "linear = SVC(kernel='linear')\n",
        "rbf = SVC(kernel='rbf')\n",
        "\n",
        "linear.fit(X_train, y_train)\n",
        "rbf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Linear Accuracy:\", linear.score(X_test, y_test))\n",
        "print(\"RBF Accuracy:\", rbf.score(X_test, y_test))\n",
        "\n",
        "3. Write a Python program to train an SVM Regressor (SVR) on a housing dataset and evaluate it using Mean Squared Error (MSE)\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "model = SVR()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
        "\n",
        "4. Write a Python program to train an SVM Classifier with a Polynomial Kernel and visualize the decision boundary\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "X, y = make_classification(n_features=2, n_redundant=0)\n",
        "\n",
        "model = SVC(kernel='poly', degree=3)\n",
        "model.fit(X, y)\n",
        "\n",
        "xx, yy = np.meshgrid(\n",
        "    np.linspace(X[:,0].min(), X[:,0].max(), 100),\n",
        "    np.linspace(X[:,1].min(), X[:,1].max(), 100)\n",
        ")\n",
        "\n",
        "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "plt.contourf(xx, yy, Z, alpha=0.3)\n",
        "plt.scatter(X[:,0], X[:,1], c=y)\n",
        "plt.show()\n",
        "\n",
        "5. Write a Python program to train a Gaussian Na√Øve Bayes classifier on the Breast Cancer dataset and evaluate accuracy\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "6. Write a Python program to train a Multinomial Na√Øve Bayes classifier for text classification using the 20 Newsgroups dataset\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = fetch_20newsgroups()\n",
        "X = CountVectorizer().fit_transform(data.data)\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "7. Write a Python program to train an SVM Classifier with different C values and compare the decision boundaries visually\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "X, y = make_classification(n_samples=100, n_features=2, n_redundant=0, n_clusters_per_class=1)\n",
        "\n",
        "C_values = [0.1, 1, 10]\n",
        "plt.figure(figsize=(12,4))\n",
        "\n",
        "for i, C in enumerate(C_values):\n",
        "    model = SVC(C=C)\n",
        "    model.fit(X, y)\n",
        "    \n",
        "    xx, yy = np.meshgrid(np.linspace(X[:,0].min(), X[:,0].max(), 100),\n",
        "                         np.linspace(X[:,1].min(), X[:,1].max(), 100))\n",
        "    \n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
        "    \n",
        "    plt.subplot(1,3,i+1)\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3)\n",
        "    plt.scatter(X[:,0], X[:,1], c=y)\n",
        "    plt.title(f\"C={C}\")\n",
        "plt.show()\n",
        "\n",
        "8. Write a Python program to train a Bernoulli Na√Øve Bayes classifier for binary classification on a dataset with binary features\n",
        "\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "X, y = make_classification(n_samples=100, n_features=10, n_informative=5, n_redundant=0, n_classes=2)\n",
        "X = (X > 0).astype(int)  # convert features to binary\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "model = BernoulliNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "9. Write a Python program to apply feature scaling before training an SVM model and compare results with unscaled data\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_wine(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "# Without scaling\n",
        "model1 = SVC()\n",
        "model1.fit(X_train, y_train)\n",
        "print(\"Accuracy without scaling:\", model1.score(X_test, y_test))\n",
        "\n",
        "# With scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model2 = SVC()\n",
        "model2.fit(X_train_scaled, y_train)\n",
        "print(\"Accuracy with scaling:\", model2.score(X_test_scaled, y_test))\n",
        "\n",
        "10. Write a Python program to train a Gaussian Na√Øve Bayes model and compare the predictions before and after Laplace Smoothing\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "# Without smoothing\n",
        "model1 = GaussianNB(var_smoothing=1e-9)\n",
        "model1.fit(X_train, y_train)\n",
        "pred1 = model1.predict(X_test)\n",
        "print(\"Accuracy without smoothing:\", accuracy_score(y_test, pred1))\n",
        "\n",
        "# With larger smoothing\n",
        "model2 = GaussianNB(var_smoothing=1e-2)\n",
        "model2.fit(X_train, y_train)\n",
        "pred2 = model2.predict(X_test)\n",
        "print(\"Accuracy with smoothing:\", accuracy_score(y_test, pred2))\n",
        "\n",
        "11. Write a Python program to train an SVM Classifier and use GridSearchCV to tune the hyperparameters (C, gamma, kernel)\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "param_grid = {'C':[0.1,1,10], 'gamma':[0.01,0.1,1], 'kernel':['linear','rbf']}\n",
        "grid = GridSearchCV(SVC(), param_grid, cv=3)\n",
        "grid.fit(X, y)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Accuracy:\", grid.best_score_)\n",
        "\n",
        "12. Write a Python program to train an SVM Classifier on an imbalanced dataset, apply class weighting and check if accuracy improves\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = make_classification(n_samples=200, weights=[0.9,0.1], n_classes=2, n_features=5)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "# Without class_weight\n",
        "model1 = SVC()\n",
        "model1.fit(X_train, y_train)\n",
        "print(\"Accuracy without class_weight:\", model1.score(X_test, y_test))\n",
        "\n",
        "# With class_weight='balanced'\n",
        "model2 = SVC(class_weight='balanced')\n",
        "model2.fit(X_train, y_train)\n",
        "print(\"Accuracy with class_weight:\", model2.score(X_test, y_test))\n",
        "\n",
        "13. Write a Python program to implement a Na√Øve Bayes classifier for spam detection using email data\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Example email data\n",
        "emails = [\"Win money now\", \"Hi, how are you?\", \"Limited offer just for you\", \"Meeting at 10am\"]\n",
        "labels = [1, 0, 1, 0]  # 1=spam, 0=ham\n",
        "\n",
        "X = CountVectorizer().fit_transform(emails)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.25)\n",
        "\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "14. Write a Python program to train an SVM Classifier and a Na√Øve Bayes Classifier on the same dataset and compare their accuracy\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "svm_model = SVC()\n",
        "svm_model.fit(X_train, y_train)\n",
        "print(\"SVM Accuracy:\", svm_model.score(X_test, y_test))\n",
        "\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "print(\"Na√Øve Bayes Accuracy:\", nb_model.score(X_test, y_test))\n",
        "\n",
        "15. Write a Python program to perform feature selection before training a Na√Øve Bayes classifier and compare results\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "# Without feature selection\n",
        "nb1 = GaussianNB()\n",
        "nb1.fit(X_train, y_train)\n",
        "print(\"Accuracy without feature selection:\", nb1.score(X_test, y_test))\n",
        "\n",
        "# With feature selection (select top 2 features)\n",
        "selector = SelectKBest(f_classif, k=2)\n",
        "X_train_new = selector.fit_transform(X_train, y_train)\n",
        "X_test_new = selector.transform(X_test)\n",
        "\n",
        "nb2 = GaussianNB()\n",
        "nb2.fit(X_train_new, y_train)\n",
        "print(\"Accuracy with feature selection:\", nb2.score(X_test_new, y_test))\n",
        "\n",
        "16. Write a Python program to train an SVM Classifier using One-vs-Rest (OvR) and One-vs-One (OvO) strategies on the Wine dataset and compare their accuracy\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = load_wine(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "ovr_model = OneVsRestClassifier(SVC())\n",
        "ovr_model.fit(X_train, y_train)\n",
        "print(\"OvR Accuracy:\", ovr_model.score(X_test, y_test))\n",
        "\n",
        "ovo_model = OneVsOneClassifier(SVC())\n",
        "ovo_model.fit(X_train, y_train)\n",
        "print(\"OvO Accuracy:\", ovo_model.score(X_test, y_test))\n",
        "\n",
        "17. Write a Python program to train an SVM Classifier using Linear, Polynomial, and RBF kernels on the Breast Cancer dataset and compare their accuracy\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "for kernel in ['linear', 'poly', 'rbf']:\n",
        "    model = SVC(kernel=kernel)\n",
        "    model.fit(X_train, y_train)\n",
        "    print(f\"{kernel} kernel accuracy:\", model.score(X_test, y_test))\n",
        "\n",
        "18. Write a Python program to train an SVM Classifier using Stratified K-Fold Cross-Validation and compute the average accuracy\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
        "\n",
        "model = SVC()\n",
        "scores = cross_val_score(model, X, y, cv=skf)\n",
        "print(\"Average Accuracy:\", scores.mean())\n",
        "\n",
        "19. Write a Python program to train a Na√Øve Bayes classifier using different prior probabilities and compare performance\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "# Without specifying priors\n",
        "nb1 = GaussianNB()\n",
        "nb1.fit(X_train, y_train)\n",
        "print(\"Accuracy without priors:\", nb1.score(X_test, y_test))\n",
        "\n",
        "# With custom priors\n",
        "priors = [0.5, 0.3, 0.2]\n",
        "nb2 = GaussianNB(priors=priors)\n",
        "nb2.fit(X_train, y_train)\n",
        "print(\"Accuracy with priors:\", nb2.score(X_test, y_test))\n",
        "\n",
        "20. Write a Python program to perform Recursive Feature Elimination (RFE) before training an SVM Classifier and compare accuracy\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = load_wine(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "# SVM without feature selection\n",
        "svm1 = SVC()\n",
        "svm1.fit(X_train, y_train)\n",
        "print(\"Accuracy without RFE:\", svm1.score(X_test, y_test))\n",
        "\n",
        "# With RFE (select top 5 features)\n",
        "selector = RFE(SVC(), n_features_to_select=5)\n",
        "X_train_rfe = selector.fit_transform(X_train, y_train)\n",
        "X_test_rfe = selector.transform(X_test)\n",
        "\n",
        "svm2 = SVC()\n",
        "svm2.fit(X_train_rfe, y_train)\n",
        "print(\"Accuracy with RFE:\", svm2.score(X_test_rfe, y_test))\n"
      ],
      "metadata": {
        "id": "lv2gaRTibGjz"
      }
    }
  ]
}