{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1️ What is unsupervised learning in the context of machine learning?\n",
        "\n",
        "Unsupervised learning is a type of machine learning where the model is trained on data without labeled outputs. The goal is to discover hidden patterns, structures, or groupings within the data. Common tasks include clustering (grouping similar data points), dimensionality reduction, and anomaly detection. Unlike supervised learning, there is no ground truth label to guide the learning process.\n",
        "\n",
        "2️ How does the K-Means clustering algorithm work?\n",
        "\n",
        "K-Means partitions data into K clusters by minimizing the distance between data points and their assigned cluster centroid.\n",
        "Steps:\n",
        "\n",
        "Initialize K centroids randomly.\n",
        "\n",
        "Assign each data point to the nearest centroid.\n",
        "\n",
        "Recompute centroids as the mean of assigned points.\n",
        "\n",
        "Repeat assignment and centroid update until convergence (centroids stop changing).\n",
        "The algorithm aims to minimize intra-cluster variance (distance within clusters).\n",
        "\n",
        "3️ Explain the concept of a dendrogram in hierarchical clustering.\n",
        "\n",
        "A dendrogram is a tree-like diagram used to visualize the hierarchical relationships between clusters. It shows how clusters are merged (or split) at different distance thresholds. The vertical axis represents distance (dissimilarity), and the horizontal axis shows data points or clusters. Cutting the dendrogram at a certain height determines the number of clusters.\n",
        "\n",
        "4️ What is the main difference between K-Means and Hierarchical Clustering?\n",
        "\n",
        "K-Means: Partition-based, requires predefined number of clusters (K), efficient for large datasets.\n",
        "\n",
        "Hierarchical Clustering: Builds a hierarchy of clusters (tree structure), does not require specifying K beforehand.\n",
        "K-Means is faster but assumes spherical clusters, while hierarchical clustering is more flexible but computationally expensive.\n",
        "\n",
        "5️ What are the advantages of DBSCAN over K-Means?\n",
        "\n",
        "DBSCAN can:\n",
        "\n",
        "Detect clusters of arbitrary shapes (not just spherical).\n",
        "\n",
        "Identify noise and outliers automatically.\n",
        "\n",
        "Work well when clusters have irregular boundaries.\n",
        "Unlike K-Means, it does not require specifying the number of clusters beforehand.\n",
        "\n",
        "6️ When would you use Silhouette Score in clustering?\n",
        "\n",
        "Silhouette Score is used to evaluate clustering quality. It measures how similar a point is to its own cluster compared to other clusters. Use it when choosing the optimal number of clusters or comparing clustering algorithms. Higher score (close to 1) indicates well-separated clusters.\n",
        "\n",
        "7️ What are the limitations of Hierarchical Clustering?\n",
        "\n",
        "Computationally expensive for large datasets (O(n²)).\n",
        "\n",
        "Once merged or split, decisions cannot be reversed.\n",
        "\n",
        "Sensitive to noise and outliers.\n",
        "\n",
        "Difficult to scale compared to algorithms like K-Means.\n",
        "\n",
        "8️ Why is feature scaling important in clustering algorithms like K-Means?\n",
        "\n",
        "K-Means relies on distance calculations (Euclidean distance). If features have different scales (e.g., age vs salary), larger scale features dominate the distance calculation, leading to biased clustering. Scaling ensures each feature contributes equally.\n",
        "\n",
        "9️ How does DBSCAN identify noise points?\n",
        "\n",
        "DBSCAN labels points as noise if they do not belong to any dense region. A point is considered noise if:\n",
        "\n",
        "It has fewer than min_samples neighbors within radius eps.\n",
        "Such points are marked with label -1.\n",
        "\n",
        "10 Define inertia in the context of K-Means.\n",
        "\n",
        "Inertia is the sum of squared distances between each data point and its assigned cluster centroid. It measures cluster compactness. Lower inertia indicates tighter, more cohesive clusters.\n",
        "\n",
        "1️1️ What is the elbow method in K-Means clustering?\n",
        "\n",
        "The elbow method helps determine the optimal number of clusters (K). It plots inertia vs K. As K increases, inertia decreases. The “elbow point” where the decrease slows significantly indicates the best K.\n",
        "\n",
        "1️2️ Describe the concept of \"density\" in DBSCAN.\n",
        "\n",
        "Density refers to the number of points within a specified radius (eps). DBSCAN forms clusters where points are densely packed and separates sparse regions as noise. Dense regions create meaningful clusters.\n",
        "\n",
        "1️3️ Can hierarchical clustering be used on categorical data?\n",
        "\n",
        "Yes, but it requires appropriate distance metrics such as Hamming distance or similarity measures instead of Euclidean distance. Standard implementations usually assume numeric data.\n",
        "\n",
        "1️4️ What does a negative Silhouette Score indicate?\n",
        "\n",
        "A negative Silhouette Score means a data point is closer to another cluster than its own cluster. This indicates poor clustering and possible misclassification of that point.\n",
        "\n",
        "1️5️ Explain the term \"linkage criteria\" in hierarchical clustering.\n",
        "\n",
        "Linkage criteria define how distance between clusters is computed during merging:\n",
        "\n",
        "Single linkage: minimum distance between points\n",
        "\n",
        "Complete linkage: maximum distance\n",
        "\n",
        "Average linkage: mean distance\n",
        "Different linkage methods produce different cluster shapes and structures.\n",
        "\n",
        "1️6️ Why might K-Means perform poorly on data with varying cluster sizes or densities?\n",
        "\n",
        "K-Means assumes clusters are spherical and similar in size. When clusters vary in size, density, or shape, centroids may shift incorrectly, causing misclassification and inaccurate cluster boundaries.\n",
        "\n",
        "1️7️ What are the core parameters in DBSCAN, and how do they influence clustering?\n",
        "\n",
        "Core parameters:\n",
        "\n",
        "eps: radius defining neighborhood size\n",
        "\n",
        "min_samples: minimum number of points to form a dense region\n",
        "Large eps → fewer clusters, more merging\n",
        "Small eps → more clusters, possibly more noise\n",
        "Higher min_samples → stricter density requirement\n",
        "\n",
        "1️8️ How does K-Means++ improve upon standard K-Means initialization?\n",
        "\n",
        "K-Means++ selects initial centroids more intelligently by spreading them apart. This reduces poor initialization, speeds up convergence, and often leads to better clustering results compared to random initialization.\n",
        "\n",
        "1️9️ What is agglomerative clustering?\n",
        "\n",
        "Agglomerative clustering is a bottom-up hierarchical clustering method. Each data point starts as its own cluster, and the algorithm repeatedly merges the closest clusters until a stopping criterion (like number of clusters) is reached.\n",
        "\n",
        "2️0️ What makes Silhouette Score a better metric than just inertia for model evaluation?\n",
        "\n",
        "Inertia only measures compactness within clusters but ignores separation between clusters. Silhouette Score considers both:\n",
        "\n",
        "Cohesion (how close points are within cluster)\n",
        "\n",
        "Separation (how far clusters are from each other)\n",
        "Hence, Silhouette Score gives a more balanced and reliable evaluation of clustering quality than inertia alone.\n",
        "\n"
      ],
      "metadata": {
        "id": "Z0zBX_AJUAnX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1️ Question:\n",
        "\n",
        "Generate synthetic data with 4 centers using make_blobs and apply K-Means clustering. Visualize using a scatter plot.\n",
        "\n",
        "Answer:\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, _ = make_blobs(n_samples=300, centers=4, random_state=42)\n",
        "\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "labels = kmeans.fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels)\n",
        "plt.title(\"KMeans Clustering (4 centers)\")\n",
        "plt.show()\n",
        "\n",
        "2️ Question:\n",
        "\n",
        "Load the Iris dataset and use Agglomerative Clustering to group the data into 3 clusters. Display the first 10 predicted labels.\n",
        "\n",
        "Answer:\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "agg = AgglomerativeClustering(n_clusters=3)\n",
        "labels = agg.fit_predict(X)\n",
        "\n",
        "print(labels[:10])\n",
        "\n",
        "3️ Question:\n",
        "\n",
        "Generate synthetic data using make_moons and apply DBSCAN. Highlight outliers in the plot.\n",
        "\n",
        "Answer:\n",
        "\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.cluster import DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, _ = make_moons(n_samples=300, noise=0.05, random_state=42)\n",
        "\n",
        "db = DBSCAN(eps=0.2, min_samples=5)\n",
        "labels = db.fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels)\n",
        "plt.title(\"DBSCAN on Moons (Outliers = -1)\")\n",
        "plt.show()\n",
        "\n",
        "4️ Question:\n",
        "\n",
        "Load the Wine dataset and apply K-Means clustering after standardizing the features. Print the size of each cluster.\n",
        "\n",
        "Answer:\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "wine = load_wine()\n",
        "X = StandardScaler().fit_transform(wine.data)\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "labels = kmeans.fit_predict(X)\n",
        "\n",
        "unique, counts = np.unique(labels, return_counts=True)\n",
        "print(dict(zip(unique, counts)))\n",
        "\n",
        "5️ Question:\n",
        "\n",
        "Use make_circles to generate synthetic data and cluster it using DBSCAN. Plot the result.\n",
        "\n",
        "Answer:\n",
        "\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.cluster import DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, _ = make_circles(n_samples=300, factor=0.5, noise=0.05)\n",
        "\n",
        "db = DBSCAN(eps=0.2, min_samples=5)\n",
        "labels = db.fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels)\n",
        "plt.title(\"DBSCAN on Circles\")\n",
        "plt.show()\n",
        "\n",
        "6️ Question:\n",
        "\n",
        "Load the Breast Cancer dataset, apply MinMaxScaler, and use K-Means with 2 clusters. Output the cluster centroids.\n",
        "\n",
        "Answer:\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = MinMaxScaler().fit_transform(data.data)\n",
        "\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "kmeans.fit(X)\n",
        "\n",
        "print(kmeans.cluster_centers_)\n",
        "\n",
        "7️ Question:\n",
        "\n",
        "Generate synthetic data using make_blobs with varying cluster standard deviations and cluster with DBSCAN.\n",
        "\n",
        "Answer:\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, _ = make_blobs(n_samples=300, centers=3, cluster_std=[1.0, 2.5, 0.5], random_state=42)\n",
        "\n",
        "db = DBSCAN(eps=0.9, min_samples=5)\n",
        "labels = db.fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels)\n",
        "plt.title(\"DBSCAN with varying std\")\n",
        "plt.show()\n",
        "\n",
        "8️ Question:\n",
        "\n",
        "Load the Digits dataset, reduce it to 2D using PCA, and visualize clusters from K-Means.\n",
        "\n",
        "Answer:\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "kmeans = KMeans(n_clusters=10, random_state=42)\n",
        "labels = kmeans.fit_predict(X_pca)\n",
        "\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels)\n",
        "plt.title(\"KMeans on Digits PCA\")\n",
        "plt.show()\n",
        "\n",
        "9 Question:\n",
        "\n",
        "Create synthetic data using make_blobs and evaluate silhouette scores for k = 2 to 5. Display as a bar chart.\n",
        "\n",
        "Answer:\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, _ = make_blobs(n_samples=300, centers=4, random_state=42)\n",
        "\n",
        "scores = []\n",
        "ks = range(2, 6)\n",
        "\n",
        "for k in ks:\n",
        "    labels = KMeans(n_clusters=k, random_state=42).fit_predict(X)\n",
        "    scores.append(silhouette_score(X, labels))\n",
        "\n",
        "plt.bar(ks, scores)\n",
        "plt.xlabel(\"k\")\n",
        "plt.ylabel(\"Silhouette Score\")\n",
        "plt.title(\"Silhouette Scores for k=2 to 5\")\n",
        "plt.show()\n",
        "\n",
        " 10.Question:\n",
        "\n",
        "Load the Iris dataset and use hierarchical clustering. Plot a dendrogram with average linkage.\n",
        "\n",
        "Answer:\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "iris = load_iris()\n",
        "Z = linkage(iris.data, method='average')\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "dendrogram(Z)\n",
        "plt.title(\"Dendrogram (Average Linkage)\")\n",
        "plt.show()\n",
        "\n",
        "1️1️ Question:\n",
        "\n",
        "Generate synthetic data with overlapping clusters using make_blobs, then apply K-Means and visualize with decision boundaries.\n",
        "\n",
        "Answer:\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "X, _ = make_blobs(n_samples=300, centers=3, cluster_std=2.5, random_state=42)\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "labels = kmeans.fit_predict(X)\n",
        "\n",
        "# Decision boundary\n",
        "h = 0.02\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "\n",
        "Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "plt.contourf(xx, yy, Z, alpha=0.3)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels)\n",
        "plt.title(\"KMeans with Decision Boundaries\")\n",
        "plt.show()\n",
        "\n",
        "1️2️ Question:\n",
        "\n",
        "Load the Digits dataset and apply DBSCAN after reducing dimensions with t-SNE. Visualize the results.\n",
        "\n",
        "Answer:\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "X_2d = tsne.fit_transform(X)\n",
        "\n",
        "db = DBSCAN(eps=5, min_samples=5)\n",
        "labels = db.fit_predict(X_2d)\n",
        "\n",
        "plt.scatter(X_2d[:, 0], X_2d[:, 1], c=labels)\n",
        "plt.title(\"DBSCAN on t-SNE Digits\")\n",
        "plt.show()\n",
        "\n",
        "1️3️ Question:\n",
        "\n",
        "Generate synthetic data using make_blobs and apply Agglomerative Clustering with complete linkage. Plot the result.\n",
        "\n",
        "Answer:\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, _ = make_blobs(n_samples=300, centers=4, random_state=42)\n",
        "\n",
        "agg = AgglomerativeClustering(n_clusters=4, linkage='complete')\n",
        "labels = agg.fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels)\n",
        "plt.title(\"Agglomerative Clustering (Complete Linkage)\")\n",
        "plt.show()\n",
        "\n",
        "1️4️ Question:\n",
        "\n",
        "Load the Breast Cancer dataset and compare inertia values for K = 2 to 6 using K-Means. Show results in a line plot.\n",
        "\n",
        "Answer:\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "\n",
        "inertia = []\n",
        "K = range(2, 7)\n",
        "\n",
        "for k in K:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(X)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "plt.plot(K, inertia, marker='o')\n",
        "plt.xlabel(\"Number of Clusters\")\n",
        "plt.ylabel(\"Inertia\")\n",
        "plt.title(\"Inertia vs K\")\n",
        "plt.show()\n",
        "\n",
        "1️5️ Question:\n",
        "\n",
        "Generate synthetic concentric circles using make_circles and cluster using Agglomerative Clustering with single linkage.\n",
        "\n",
        "Answer:\n",
        "\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, _ = make_circles(n_samples=300, factor=0.5, noise=0.05)\n",
        "\n",
        "agg = AgglomerativeClustering(n_clusters=2, linkage='single')\n",
        "labels = agg.fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels)\n",
        "plt.title(\"Agglomerative Clustering (Single Linkage)\")\n",
        "plt.show()\n",
        "\n",
        "1️6️ Question:\n",
        "\n",
        "Use the Wine dataset, apply DBSCAN after scaling the data, and count the number of clusters (excluding noise).\n",
        "\n",
        "Answer:\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "\n",
        "wine = load_wine()\n",
        "X = StandardScaler().fit_transform(wine.data)\n",
        "\n",
        "db = DBSCAN(eps=1.5, min_samples=5)\n",
        "labels = db.fit_predict(X)\n",
        "\n",
        "clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "print(\"Number of clusters (excluding noise):\", clusters)\n",
        "\n",
        "1️7️ Question:\n",
        "\n",
        "Generate synthetic data with make_blobs and apply KMeans. Then plot the cluster centers on top of the data points.\n",
        "\n",
        "Answer:\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, _ = make_blobs(n_samples=300, centers=3, random_state=42)\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "labels = kmeans.fit_predict(X)\n",
        "\n",
        "centers = kmeans.cluster_centers_\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels)\n",
        "plt.scatter(centers[:, 0], centers[:, 1], s=200, marker='X')\n",
        "plt.title(\"KMeans with Cluster Centers\")\n",
        "plt.show()\n",
        "\n",
        "1️8️ Question:\n",
        "\n",
        "Load the Iris dataset, cluster with DBSCAN, and print how many samples were identified as noise.\n",
        "\n",
        "Answer:\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "db = DBSCAN(eps=0.5, min_samples=5)\n",
        "labels = db.fit_predict(X)\n",
        "\n",
        "noise_count = list(labels).count(-1)\n",
        "print(\"Number of noise samples:\", noise_count)\n",
        "\n",
        "1️9️ Question:\n",
        "\n",
        "Generate synthetic non-linearly separable data using make_moons, apply K-Means, and visualize the clustering result.\n",
        "\n",
        "Answer:\n",
        "\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, _ = make_moons(n_samples=300, noise=0.05, random_state=42)\n",
        "\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "labels = kmeans.fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels)\n",
        "plt.title(\"KMeans on Non-linear Moons Data\")\n",
        "plt.show()\n",
        "\n",
        "2️0️ Question:\n",
        "\n",
        "Load the Digits dataset, apply PCA to reduce to 3 components, then use KMeans and visualize with a 3D scatter plot.\n",
        "\n",
        "Answer:\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "\n",
        "pca = PCA(n_components=3)\n",
        "X_3d = pca.fit_transform(X)\n",
        "\n",
        "kmeans = KMeans(n_clusters=10, random_state=42)\n",
        "labels = kmeans.fit_predict(X_3d)\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(X_3d[:, 0], X_3d[:, 1], X_3d[:, 2], c=labels)\n",
        "ax.set_title(\"3D PCA + KMeans on Digits\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "oM-MxKXBSEZl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ISrNEtGRT-vP"
      }
    }
  ]
}