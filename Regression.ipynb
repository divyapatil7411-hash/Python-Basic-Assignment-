{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaDl4695lqKT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Simple Linear Regression?\n",
        "\n",
        "Simple Linear Regression models the relationship between one predictor X and one outcome Y using a straight line:\n",
        "\n",
        "Y = mX + c + ε\n",
        "\n",
        "Example: Predict salary from years of experience.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "X = np.array([0,1,2,3,4,5]).reshape(-1,1)\n",
        "y = 30000 + 5000*X.ravel() + np.random.normal(0, 3000, size=X.shape[0])\n",
        "model = LinearRegression().fit(X, y)\n",
        "print(\"Slope:\", model.coef_[0], \"Intercept:\", model.intercept_)\n",
        "\n",
        "2. Key assumptions of Simple Linear Regression\n",
        "\n",
        "Linearity\n",
        "\n",
        "Independence of residuals\n",
        "\n",
        "Homoscedasticity\n",
        "\n",
        "Normal distribution of residuals\n",
        "\n",
        "No major outliers\n",
        "\n",
        "Example: House price predicted from size assumes linearity.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "X = np.linspace(500, 2500, 60).reshape(-1,1)\n",
        "y = 2000*X.ravel() + 100000 + np.random.normal(0, 80000, size=X.shape[0])\n",
        "plt.scatter(X,y)\n",
        "plt.show()\n",
        "\n",
        "3. Coefficient m in Y = mx + c\n",
        "\n",
        "Slope m = change in Y for a one-unit increase in X. Example: Each extra square foot increases house price by ₹2000.\n",
        "\n",
        "4. Intercept c in Y = mx + c\n",
        "\n",
        "Intercept c = expected Y when X=0. Example: Starting salary at 0 years experience.\n",
        "\n",
        "5. Calculating slope m\n",
        "\n",
        "m = Σ(Xi - X̄)(Yi - Ȳ) / Σ(Xi - X̄)²\n",
        "c = Ȳ - mX̄\n",
        "\n",
        "Example: Study hours vs exam score.\n",
        "\n",
        "6. Purpose of least squares method\n",
        "\n",
        "Minimizes squared residuals to find best-fit line. Example: Fit line for ad spend vs sales.\n",
        "\n",
        "7. R² interpretation\n",
        "\n",
        "Proportion of variance in Y explained by X. Example: R² = 0.85 means 85% of salary variation explained by experience.\n",
        "\n",
        "8. Multiple Linear Regression\n",
        "\n",
        "Models Y using multiple predictors.\n",
        "\n",
        "Y = β0 + β1X1 + β2X2 + ... + βpXp + ε\n",
        "\n",
        "Example: House price ~ size + bedrooms + location.\n",
        "\n",
        "9. Difference between Simple and Multiple Linear Regression\n",
        "\n",
        "Simple: one predictor\n",
        "\n",
        "Multiple: two or more predictors\n",
        "\n",
        "10. Key assumptions of Multiple Linear Regression\n",
        "\n",
        "Linearity\n",
        "\n",
        "Independence\n",
        "\n",
        "Homoscedasticity\n",
        "\n",
        "Normal residuals\n",
        "\n",
        "No multicollinearity\n",
        "\n",
        "11. Heteroscedasticity\n",
        "\n",
        "Residuals have unequal variance. Coefficients unbiased, but SE unreliable. Example: Income variance increases with age.\n",
        "\n",
        "12. Improving multicollinearity\n",
        "\n",
        "Remove correlated predictors\n",
        "\n",
        "Use Ridge/Lasso\n",
        "\n",
        "Apply PCA\n",
        "\n",
        "13. Transforming categorical variables\n",
        "\n",
        "One-hot encoding\n",
        "\n",
        "Ordinal encoding\n",
        "\n",
        "Target encoding\n",
        "\n",
        "14. Interaction terms\n",
        "\n",
        "Capture combined effects.\n",
        "\n",
        "Y = β0 + β1X1 + β2X2 + β3(X1X2)\n",
        "\n",
        "Example: Study hours effect depends on sleep quality.\n",
        "\n",
        "15. Intercept interpretation differences\n",
        "\n",
        "Simple: Y when X=0\n",
        "\n",
        "Multiple: Y when all predictors=0\n",
        "\n",
        "16. Significance of slope\n",
        "\n",
        "Shows how much Y changes per unit of X. Example: Advertising spend slope = extra sales per ₹1000.\n",
        "\n",
        "17. Intercept context\n",
        "\n",
        "Baseline value of Y when predictors=0. Example: Exam score at 0 study hours.\n",
        "\n",
        "18. Limitations of R²\n",
        "\n",
        "Increases with more predictors\n",
        "\n",
        "Ignores diagnostics\n",
        "\n",
        "Doesn’t measure predictive accuracy\n",
        "\n",
        "19. Large standard error\n",
        "\n",
        "Coefficient estimate is uncertain. Example: Education level coefficient in salary model unclear.\n",
        "\n",
        "20. Identifying heteroscedasticity\n",
        "\n",
        "Residual vs fitted plot shows funnel shape. Example: Car price residuals spread wider for expensive cars.\n",
        "\n",
        "21. High R² but low adjusted R²\n",
        "\n",
        "Extra predictors inflate R² without real value.\n",
        "\n",
        "22. Importance of scaling\n",
        "\n",
        "Numerical stability\n",
        "\n",
        "Comparability\n",
        "\n",
        "Needed for Ridge/Lasso\n",
        "\n",
        "23. Polynomial regression\n",
        "\n",
        "Regression with powers of predictors.\n",
        "\n",
        "Y = β0 + β1X + β2X² + ... + βdXd\n",
        "\n",
        "24. Difference from linear regression\n",
        "\n",
        "Linear fits straight line; polynomial fits curves.\n",
        "\n",
        "25. When polynomial regression is used\n",
        "\n",
        "When data shows nonlinear patterns. Example: Fertilizer vs crop yield.\n",
        "\n",
        "26. General equation\n",
        "\n",
        "Y = β0 + Σ βkX^k + ε\n",
        "\n",
        "27. Polynomial regression with multiple variables\n",
        "\n",
        "Yes, include polynomial terms and interactions.\n",
        "\n",
        "28. Limitations of polynomial regression\n",
        "\n",
        "Overfitting\n",
        "\n",
        "Poor extrapolation\n",
        "\n",
        "Hard to interpret coefficients\n",
        "\n",
        "29. Evaluating polynomial degree\n",
        "\n",
        "Cross-validation\n",
        "\n",
        "Adjusted R², AIC, BIC\n",
        "\n",
        "Residual plots\n",
        "\n",
        "30. Importance of visualization\n",
        "\n",
        "Shows curve fit, detects overfitting, communicates results.\n",
        "\n",
        "31. Polynomial regression in Python\n",
        "\n",
        "Use PolynomialFeatures + LinearRegression.\n",
        "\n",
        "import numpy as np, matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "X = np.linspace(0, 6, 50).reshape(-1,1)\n",
        "y = np.sin(X).ravel() + np.random.normal(0, 0.2, 50)\n",
        "\n",
        "model = Pipeline([\n",
        "    ('poly', PolynomialFeatures(degree=3, include_bias=False)),\n",
        "    ('lin', LinearRegression())\n",
        "])\n",
        "model.fit(X, y)\n",
        "\n",
        "X_plot = np.linspace(0, 6, 200).reshape(-1,1)\n",
        "Y_pred = model.predict(X_plot)\n",
        "plt.scatter(X, y)\n",
        "plt.plot(X_plot, Y_pred, color='red')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8M78FsV1l6FR"
      }
    }
  ]
}