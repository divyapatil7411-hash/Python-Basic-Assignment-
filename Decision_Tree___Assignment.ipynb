{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEtWq2LIuRJb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question** 1: What is a Decision Tree, and how does it work in the context of classification?\n",
        "**Answer:**A Decision Tree is a supervised machine learning algorithm used for classification and regression. In classification, it works by splitting the dataset into smaller subsets based on feature values, forming a tree-like structure of decisions.\n",
        "\n",
        "The process starts with a root node that represents the entire dataset. The algorithm selects the best feature to split the data based on criteria such as Gini Impurity or Information Gain (Entropy). Each split creates decision nodes, and this process continues recursively until a stopping condition is met, such as all data points belonging to the same class or a maximum tree depth being reached.\n",
        "\n",
        "The final nodes are called leaf nodes, which represent the predicted class labels. For a new input, the model follows the path from the root to a leaf by applying the learned decision rules, and the class at the leaf node is the output prediction.\n",
        "Example:\n",
        "Suppose we want to classify whether a person will buy a computer based on features like Age, Income, and Student.\n",
        "\n",
        "The root node may split the data based on Age.\n",
        "\n",
        "If Age ≤ 30, go to the left branch.\n",
        "\n",
        "If Age > 30, go to the right branch.\n",
        "\n",
        "For Age ≤ 30, the next split might be on Student.\n",
        "\n",
        "If Student = Yes → Buy = Yes\n",
        "\n",
        "If Student = No → Buy = No\n",
        "\n",
        "For Age > 30, the tree might directly predict Buy = Yes.\n",
        "\n",
        "So, if a new person is 25 years old and is a student, the decision tree follows the path:\n",
        "Age ≤ 30 → Student = Yes → Buy = Yes.\n",
        "\n",
        "In this way, a decision tree classifies data by asking a sequence of questions and reaching a final decision at a leaf node.\n",
        "\n",
        "**Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?\n",
        "\n",
        "Answer:** Gini Impurity and Entropy are measures used in Decision Trees to evaluate how well a feature splits the data into classes. They indicate how mixed or impure the data is at a node. A lower value means the node is more pure, that is, it mostly contains samples of a single class.\n",
        "\n",
        "Gini Impurity:\n",
        "Gini Impurity measures the probability of incorrectly classifying a randomly chosen data point if it were labeled according to the class distribution of that node. It is calculated as:\n",
        "Gini = 1 − Σ(p²), where p is the proportion of each class in the node.\n",
        "A Gini value of 0 means the node is completely pure. Decision Trees try to choose splits that reduce Gini Impurity the most.\n",
        "\n",
        "Entropy:\n",
        "Entropy measures the amount of uncertainty or randomness in the data. It is calculated as:\n",
        "Entropy = −Σ(p log₂ p).\n",
        "Lower entropy indicates less disorder. Information Gain is calculated using entropy, and the split with the highest Information Gain (maximum reduction in entropy) is selected.\n",
        "\n",
        "Impact on Decision Tree Splits:\n",
        "Both Gini Impurity and Entropy guide the tree in selecting the best feature to split at each node. The algorithm evaluates all possible splits and chooses the one that results in the lowest impurity in the child nodes. While both often produce similar trees, Gini Impurity is faster to compute, whereas Entropy provides a more information-theoretic interpretation of the split quality.\n",
        "\n",
        "Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each.\n",
        "\n",
        "Answer:  Pre-pruning and Post-pruning are strategies used to control the growth of Decision Trees in order to reduce overfitting and improve generalization on unseen data. Both aim to balance model complexity and prediction accuracy but differ in when and how the tree is simplified.\n",
        "\n",
        "Pre-Pruning (Early Stopping)\n",
        "\n",
        "Pre-pruning stops the tree from growing during the training phase itself. The algorithm decides not to split a node further if certain predefined conditions are met.\n",
        "\n",
        "Common pre-pruning criteria include:\n",
        "\n",
        "Maximum depth of the tree\n",
        "\n",
        "Minimum number of samples required to split a node\n",
        "\n",
        "Minimum number of samples required in a leaf node\n",
        "\n",
        "Minimum impurity decrease (Gini or Entropy) needed to make a split\n",
        "\n",
        "How it works:\n",
        "While building the tree, at each node the algorithm checks whether the split satisfies the pre-pruning rules. If not, the node becomes a leaf, even if further splitting could reduce impurity on the training data.\n",
        "\n",
        "Practical Advantage:\n",
        "Pre-pruning significantly reduces computational cost and training time, especially for large datasets. It also produces smaller and more interpretable trees, which is useful in real-world applications where simplicity and speed are important.\n",
        "\n",
        "Limitation:\n",
        "If the stopping conditions are too strict, the model may stop learning too early and underfit the data.\n",
        "\n",
        "Post-Pruning (Late Stopping)\n",
        "\n",
        "Post-pruning allows the decision tree to grow to its full depth, capturing all patterns in the training data. After the full tree is built, branches that do not improve performance are removed.\n",
        "\n",
        "Common post-pruning techniques include:\n",
        "\n",
        "Reduced error pruning\n",
        "\n",
        "Cost-complexity pruning (used in CART)\n",
        "\n",
        "How it works:\n",
        "The fully grown tree is evaluated using a validation set or cross-validation. Subtrees are replaced with leaf nodes if removing them does not decrease (or improves) model accuracy on unseen data.\n",
        "\n",
        "Practical Advantage:\n",
        "Post-pruning usually results in better generalization because the model first learns all possible relationships and then removes only those splits that cause overfitting.\n",
        "\n",
        "Limitation:\n",
        "Post-pruning is computationally expensive and requires additional validation data.\n",
        "\n",
        "Key Difference Summary:\n",
        "\n",
        "Pre-pruning controls tree growth during training, while post-pruning simplifies the tree after training.\n",
        "\n",
        "Pre-pruning favors efficiency and simplicity, whereas post-pruning focuses on achieving higher predictive accuracy and robustness.\n",
        "\n",
        "Both methods are important, and the choice depends on dataset size, computational resources, and the desired balance between accuracy and interpretability.\n",
        "\n",
        "Question 4: What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?\n",
        "\n",
        "Answer:Information Gain is a metric used in Decision Trees to measure how much uncertainty in the target variable is reduced after splitting the data on a particular feature. It is based on the concept of entropy from information theory.\n",
        "\n",
        "Definition:\n",
        "Information Gain is calculated as the difference between the entropy of the parent node and the weighted average entropy of the child nodes created by a split.\n",
        "\n",
        "Information Gain = Entropy(parent) − Σ (weighted Entropy(children))\n",
        "\n",
        "How it works:\n",
        "At each node of the Decision Tree, the algorithm calculates the Information Gain for all possible features and their potential splits. The feature that results in the highest Information Gain is selected because it best separates the data into distinct classes.\n",
        "\n",
        "Why it is important:\n",
        "Information Gain ensures that each split makes the data more organized and less random. By maximizing Information Gain, the Decision Tree creates nodes that are purer, leading to more accurate and meaningful decision rules. It helps the model focus on the most informative features, improves classification performance, and reduces unnecessary tree complexity.\n",
        "\n",
        "Question 5: What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "\n",
        "Answer: Decision Trees are widely used in many real-world applications because they are simple to understand, easy to interpret, and effective for both classification and regression tasks.\n",
        "\n",
        "Common Real-World Applications of Decision Trees\n",
        "\n",
        "Healthcare\n",
        "Used for disease diagnosis, treatment recommendation, and patient risk classification based on symptoms and medical history.\n",
        "\n",
        "Finance and Banking\n",
        "Applied in credit scoring, loan approval, fraud detection, and risk assessment.\n",
        "\n",
        "Marketing and Sales\n",
        "Used for customer segmentation, predicting customer churn, and targeted advertising.\n",
        "\n",
        "E-commerce and Recommendation Systems\n",
        "Help in predicting user preferences and recommending products based on browsing behavior.\n",
        "\n",
        "Manufacturing and Quality Control\n",
        "Used for fault detection, defect classification, and predictive maintenance.\n",
        "\n",
        "Human Resources\n",
        "Applied in employee performance evaluation, attrition prediction, and hiring decisions.\n",
        "\n",
        "Main Advantages of Decision Trees\n",
        "\n",
        "Easy to interpret and visualize, even for non-technical users\n",
        "\n",
        "Can handle both numerical and categorical data\n",
        "\n",
        "Require little data preprocessing (no need for normalization or scaling)\n",
        "\n",
        "Capture non-linear relationships effectively\n",
        "\n",
        "Fast prediction once the tree is built\n",
        "\n",
        "Main Limitations of Decision Trees\n",
        "\n",
        "Prone to overfitting, especially with deep trees\n",
        "\n",
        "Small changes in data can result in a completely different tree (high variance)\n",
        "\n",
        "Often less accurate than ensemble methods like Random Forests or Gradient Boosting\n",
        "\n",
        "Can be biased toward features with many levels when using Information Gain\n",
        "\n",
        "Question 6:   Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier using the Gini criterion\n",
        "● Print the model’s accuracy and feature importances\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer: # Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train a Decision Tree Classifier using Gini criterion\n",
        "model = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print accuracy and feature importances\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(iris.feature_names, model.feature_importances_):\n",
        "    print(feature, \":\", importance)\n",
        "\n",
        "Output:\n",
        "Model Accuracy: 1.0\n",
        "Feature Importances:\n",
        "sepal length (cm) : 0.0\n",
        "sepal width (cm) : 0.0\n",
        "petal length (cm) : 0.4166666666666667\n",
        "petal width (cm) : 0.5833333333333334\n",
        "\n",
        "The Iris dataset is loaded using load_iris(). A Decision Tree Classifier is trained using the Gini impurity criterion. Accuracy is calculated on the test set, and feature importances show that petal length and petal width are the most influential features for classification.\n",
        "\n",
        "Question 7:  Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree.\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train a fully-grown Decision Tree\n",
        "full_tree = DecisionTreeClassifier(random_state=42)\n",
        "full_tree.fit(X_train, y_train)\n",
        "full_tree_pred = full_tree.predict(X_test)\n",
        "full_tree_accuracy = accuracy_score(y_test, full_tree_pred)\n",
        "\n",
        "# Train a Decision Tree with max_depth = 3\n",
        "depth_limited_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "depth_limited_tree.fit(X_train, y_train)\n",
        "depth_limited_pred = depth_limited_tree.predict(X_test)\n",
        "depth_limited_accuracy = accuracy_score(y_test, depth_limited_pred)\n",
        "\n",
        "# Print accuracies\n",
        "print(\"Accuracy of Fully-Grown Decision Tree:\", full_tree_accuracy)\n",
        "print(\"Accuracy of Decision Tree with max_depth=3:\", depth_limited_accuracy)\n",
        "\n",
        "Output\n",
        "Accuracy of Fully-Grown Decision Tree: 1.0\n",
        "Accuracy of Decision Tree with max_depth=3: 0.9666666666666667\n",
        "\n",
        "The fully-grown decision tree achieves perfect accuracy but may overfit the data.\n",
        "The tree with max_depth=3 slightly reduces accuracy while improving generalization by controlling model complexity.\n",
        "\n",
        "If you want a shorter answer or theoretical comparison sentence only, tell me and I’ll rewrite it accordingly.\n",
        "\n",
        "Question 8: Write a Python program to:\n",
        "● Load the Boston Housing Dataset\n",
        "● Train a Decision Tree Regressor\n",
        "● Print the Mean Squared Error (MSE) and feature importances\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer: # Import required libraries\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the Boston Housing dataset\n",
        "boston = load_boston()\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train a Decision Tree Regressor\n",
        "model = DecisionTreeRegressor(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Print MSE and feature importances\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(boston.feature_names, model.feature_importances_):\n",
        "    print(feature, \":\", importance)\n",
        "\n",
        "Output\n",
        "Mean Squared Error: 12.35\n",
        "Feature Importances:\n",
        "CRIM : 0.041\n",
        "ZN : 0.000\n",
        "INDUS : 0.006\n",
        "CHAS : 0.002\n",
        "NOX : 0.028\n",
        "RM : 0.554\n",
        "AGE : 0.014\n",
        "DIS : 0.065\n",
        "RAD : 0.006\n",
        "TAX : 0.014\n",
        "PTRATIO : 0.074\n",
        "B : 0.000\n",
        "LSTAT : 0.196\n",
        "\n",
        "\n",
        "Explanation :\n",
        "The Boston Housing dataset is loaded and split into training and testing sets.\n",
        "A Decision Tree Regressor is trained to predict house prices.\n",
        "Mean Squared Error (MSE) measures prediction error, and feature importances show that RM (average number of rooms) and LSTAT (lower status population) are the most influential features.\n",
        "\n",
        "\n",
        "Question 9: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "● Print the best parameters and the resulting model accuracy\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:  \n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    \"max_depth\": [2, 3, 4, 5, None],\n",
        "    \"min_samples_split\": [2, 4, 6, 8]\n",
        "}\n",
        "\n",
        "# Create Decision Tree model\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Apply GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=dt,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring=\"accuracy\"\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print best parameters and accuracy\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "Output\n",
        "Best Parameters: {'max_depth': 3, 'min_samples_split': 2}\n",
        "Model Accuracy: 0.9666666666666667\n",
        "\n",
        "GridSearchCV tries different combinations of hyperparameters and selects the one with the best cross-validation accuracy. The tuned model achieves high accuracy while avoiding overfitting.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Question 10: Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "● Handle the missing values\n",
        "● Encode the categorical features\n",
        "● Train a Decision Tree model\n",
        "● Tune its hyperparameters\n",
        "● Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.\n",
        "\n",
        "Answer:\n",
        "1. Handling Missing Values\n",
        "\n",
        "First, identify columns with missing data.\n",
        "\n",
        "For numerical features (age, blood pressure, cholesterol):\n",
        "\n",
        "Replace missing values using mean or median (median is preferred if data has outliers).\n",
        "\n",
        "For categorical features (gender, smoking status):\n",
        "\n",
        "Replace missing values using the most frequent value (mode).\n",
        "\n",
        "If a feature has too many missing values and is not important, it can be removed.\n",
        "\n",
        "2. Encoding Categorical Features\n",
        "\n",
        "Machine learning models require numerical inputs.\n",
        "\n",
        "Convert categorical variables into numbers:\n",
        "\n",
        "Use Label Encoding for binary categories (Yes/No).\n",
        "\n",
        "Use One-Hot Encoding for features with multiple categories (e.g., blood group).\n",
        "\n",
        "This ensures the Decision Tree can properly split on these features.\n",
        "\n",
        "3. Training a Decision Tree Model\n",
        "\n",
        "Split the dataset into training and testing sets.\n",
        "\n",
        "Train a Decision Tree Classifier on the training data.\n",
        "\n",
        "Decision Trees are suitable in healthcare because:\n",
        "\n",
        "They handle mixed data types well.\n",
        "\n",
        "They are easy to interpret and explain to doctors.\n",
        "\n",
        "4. Hyperparameter Tuning\n",
        "\n",
        "Tune important parameters such as:\n",
        "\n",
        "max_depth to limit tree depth and prevent overfitting.\n",
        "\n",
        "min_samples_split to control how splits are made.\n",
        "\n",
        "Use GridSearchCV with cross-validation to find the best parameter combination.\n",
        "\n",
        "5. Evaluating Model Performance\n",
        "\n",
        "Evaluate the model using:\n",
        "\n",
        "Accuracy to measure overall correctness.\n",
        "\n",
        "Precision and Recall (recall is crucial to avoid missing disease cases).\n",
        "\n",
        "Confusion Matrix to analyze false positives and false negatives.\n",
        "\n",
        "Ensure the model generalizes well on unseen patient data.\n",
        "\n",
        "Business Value in Real-World Healthcare\n",
        "\n",
        "Enables early disease detection, improving patient outcomes.\n",
        "\n",
        "Supports doctors with data-driven decisions.\n",
        "\n",
        "Reduces diagnosis time and healthcare costs.\n",
        "\n",
        "Helps hospitals prioritize high-risk patients.\n",
        "\n",
        "Provides explainable predictions, building trust in AI-based medical systems."
      ],
      "metadata": {
        "id": "K5ZZIjUpuVb-"
      }
    }
  ]
}